{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Modeling Lab\n",
    "\n",
    "This lab will walk you through the basics of building a linear regression model out of a training and test set using a variety of techniques, including:\n",
    "\n",
    " - estimating distributional fit\n",
    " - onehot and target encoding\n",
    " - measuring progress with cross validation scores\n",
    " - creating a custom loss function\n",
    " - properly using inferences from the training set to transform the test set\n",
    " \n",
    "**Some of these columns might have missing values.  Decide on the best approach for filling them in based on what we did from last class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1).  Upload the training and test set from the `\\movies` folder inside the `\\Data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lreg = LinearRegression()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import probplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/devonbancroft/Desktop/DAT-10-14/class material/Unit3/Data/movies/train.csv')\n",
    "test = pd.read_csv('/Users/devonbancroft/Desktop/DAT-10-14/class material/Unit3/Data/movies/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2).  Using a Custom Loss Function\n",
    "\n",
    "To avoid some of the pitfalls of using a loss function that measures squared error, we're going to modify it a little bit.  This is also a useful skill in practice because lots of projects will require something precise that's not available out-of-the-box in a library.\n",
    "\n",
    "`Scitkit-Learn` allows for custom loss functions relatively easily\n",
    "\n",
    "We're going to instead use the **mean squared log error**.  It has the following form:\n",
    "\n",
    "$$ \\frac{\\sum{log_{e}(y - \\bar{y})^2}}{n} $$\n",
    "\n",
    "The easiest way to do this is the following:\n",
    "\n",
    " - take the log of y using `np.log1p` to avoid the hassles of dealing with negative values\n",
    " - fit your model to that, and then calculate the resulting mean squared error\n",
    " \n",
    "So your job is two fold:\n",
    " - log transform the target variable (revenue)\n",
    " - create a function called `mean_squared_log_error` according to the specifications defined here:  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html, under the heading for the `scoring` argument\n",
    " - to test that you did this correctly, run a 10-fold univariate linear regression on the training set using the `popularity` column as `X` and `revenue` as y.  The correct value should be 60.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3).  Distributional Inference of Your Continuous Variables\n",
    "\n",
    "This dataset is far from normal.  Use the `probplot()` method to find the *least* normal variable among your numeric variables, judging by the r-squared value of the resulting line.  \n",
    "\n",
    "Then, see if log-transforming improves its behavior at all.  Use a comparison between your validation scores in a univariate regression between the treated and untreated versions of the variable as your indicator of whether or not this made anything better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4).  Encoding the `Director` Column\n",
    "\n",
    "The `Director` column is a good example of some of the challenges of dealing with categorical data.  If George Lucas or Steven Spielberg direct a film, there's a good chance that has a non-random impact on a film's bottom line.  However, there are a lot of unique values, most of which are probably non-impactful.  \n",
    "\n",
    "Creating a column for everyone is probably not a good idea, but there's also no clear 'order' you could assign them just by looking at their labels.  \n",
    "\n",
    "In this step you're going to try two different techniques to see which one works better on your dataset.\n",
    "\n",
    "**Technique 1:**  Only include directors that have a value count of at least 10 *in your training set*, and set everything else to other.  \n",
    "\n",
    "So:\n",
    "\n",
    " - transform the column accordingly (you can make a new column if that's easier)\n",
    " - transform the same column in your test set so that if a director's name *doesn't* appear in your new training column it gets set to `Other`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 2:** Use target encoding to transform the column instead, and use the results from your training set to transform your test set.  There are a lot of directors in your test set that are not in your training set, and this will result in missing values.  Fill these in with the column average.\n",
    "\n",
    "**Bonus:** The method we're using here is a little blunt because our average value doesn't account for how often a particular value occurs.  A more nuanced approach to is to take some sort of weighted share between the overall column average and average of your particular unique value.  A good article on this is here:  https://maxhalford.github.io/blog/target-encoding-done-the-right-way/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold univariate regression on both to see which one gives you a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5).  Standardize Your Data using the `StandardScaler` module\n",
    "\n",
    " - make sure to `fit` it on the training set and `transform` it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6).  To get an estimate of your models performance, use 10-fold cross validation on your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7).  Now, before making your final predictions for your test sit, fit the model on all of your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8).  Make a prediction on your test set, and save the results as a dataframe, using two columns:\n",
    "\n",
    " - **id**:  the id of your test set rows\n",
    " - **prediction**: your corresponding predictions\n",
    " \n",
    "Submit this to a csv file, using the option `index=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
