{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Modeling Lab\n",
    "\n",
    "This lab will walk you through the basics of building a linear regression model out of a training and test set using a variety of techniques, including:\n",
    "\n",
    " - estimating distributional fit\n",
    " - onehot and target encoding\n",
    " - measuring progress with cross validation scores\n",
    " - creating a custom loss function\n",
    " - properly using inferences from the training set to transform the test set\n",
    " \n",
    "**Some of these columns might have missing values.  Decide on the best approach for filling them in based on what we did from last class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1).  Upload the training and test set from the `\\movies` folder in the `Data` folder.  Remove the 'id' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('../../Data/movies/train.csv', parse_dates=['release_date'])\n",
    "test  = pd.read_csv('../../Data/movies/test.csv', parse_dates=['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2).  Using a Custom Loss Function\n",
    "\n",
    "To avoid some of the pitfalls of using a loss function that measures squared error, we're going to modify it a little bit.  This is also a useful skill in practice because lots of projects will require something precise that's not available out-of-the-box in a library.\n",
    "\n",
    "`Scitkit-Learn` allows for custom loss functions relatively easily\n",
    "\n",
    "We're going to instead use the **mean squared log error**.  It has the following form:\n",
    "\n",
    "$$ \\frac{\\sum{log_{e}(y - \\bar{y})^2}}{n} $$\n",
    "\n",
    "The easiest way to do this is the following:\n",
    "\n",
    " - take the log of y using `np.log1p` to avoid the hassles of dealing with negative values\n",
    " - fit your model to that, and then calculate the resulting mean squared error\n",
    " \n",
    "So your job is two fold:\n",
    " - log transform the target variable (revenue)\n",
    " - create a function called `mean_squared_log_error` according to the specifications defined here:  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html, under the heading for the `scoring` argument\n",
    " - to test that you did this correctly, run a 10-fold univariate linear regression on the training set using the `popularity` column as `X` and `revenue` as y.  The average value of your scores should be 60.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "train['revenue'] = np.log1p(train['revenue'])\n",
    "\n",
    "def mean_squared_log_error(model, X, y):\n",
    "    # our error\n",
    "    error = model.predict(X) - y\n",
    "    # the average value of its square\n",
    "    mse = np.mean(error**2)\n",
    "    return mse\n",
    "\n",
    "lreg = LinearRegression()\n",
    "X = train[['popularity']]\n",
    "y = train['revenue']\n",
    "\n",
    "scores = cross_val_score(estimator=lreg, X=X, y=y, scoring=mean_squared_log_error, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.73027393164422"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and get their average value\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3).  Distributional Inference of Your Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from scipy.stats import probplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = train.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove('revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the r_squared values\n",
    "r_squared_vals = [probplot(train[num_cols[i]])[1][2] for i in range(len(num_cols))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vote_count'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and get which one it is\n",
    "min_idx = r_squared_vals.index(min(r_squared_vals))\n",
    "num_cols[min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now fit a regression model to determine if transforming it helps\n",
    "score1 = cross_val_score(estimator=lreg, X=train[['vote_count']], y=y, scoring=mean_squared_log_error, cv=10)\n",
    "score2 = cross_val_score(estimator=lreg, X=np.log1p(train[['vote_count']]), y=y, scoring=mean_squared_log_error, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.94013966051395, 44.4699238212142)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second version is the clear winner\n",
    "np.mean(score1), np.mean(score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4).  Encoding the `Director` Column\n",
    "\n",
    "The `Director` column is a good example of some of the challenges of dealing with categorical data.  If George Lucas or Steven Spielberg direct a film, there's a good chance that has a non-random impact on a film's bottom line.  However, there are a lot of unique values, most of which are probably non-impactful.  \n",
    "\n",
    "Creating a column for everyone is probably not a good idea, but there's also no clear 'order' you could assign them just by looking at their labels.  \n",
    "\n",
    "In this step you're going to try two different techniques to see which one works better on your dataset.\n",
    "\n",
    "**Technique 1:**  Only include directors that have a value count of at least 10 *in your training set*, and set everything else to other.  \n",
    "\n",
    "So:\n",
    "\n",
    " - transform the column accordingly (you can make a new column if that's easier)\n",
    " - transform the same column in your test set so that if a director's name *doesn't* appear in your new training column it gets set to `Other`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll assume we don't know who the director is\n",
    "train['director'].fillna('Unknown', inplace=True)\n",
    "test['director'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# how often did each director appear in the training set?\n",
    "director_counts = train.groupby('director')['director'].transform('count')\n",
    "# whenever this value is less than 10, set the value to other\n",
    "train['director1'] = np.where(director_counts > 10, train['director'], 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now transform the test set in a similar manner -- check to see if the unique values in the director column match\n",
    "# what's in the training column\n",
    "test['director1'] = np.where(test['director'].isin(train['director1']), test['director'], 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 2:** Use target encoding to transform the column instead, and use the results from your training set to transform your test set.  There are a lot of directors in your test set that are not in your training set, and this will result in missing values.  Fill these in with the column average.\n",
    "\n",
    "**Bonus:** The method we're using here is a little blunt because our average value doesn't account for how often a particular value occurs.  A more nuanced approach to is to take some sort of weighted share between the overall column average and average of your particular unique value.  A good article on this is here:  https://maxhalford.github.io/blog/target-encoding-done-the-right-way/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average value of each director's gross\n",
    "avg_director_gross = train.groupby('director')['revenue'].mean()\n",
    "# map those values to what we already have\n",
    "train['director2'] = train['director'].map(avg_director_gross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for the test column, and make sure to fill in missing values\n",
    "test['director2'] = test['director'].map(avg_director_gross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values with avg value of the revenue column\n",
    "test['director2'].fillna(train['revenue'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold univariate regression on both to see which one gives you a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "dir_scores1 = cross_val_score(estimator=lreg, X=pd.get_dummies(train['director1']), y=y, scoring=mean_squared_log_error, cv=10)\n",
    "dir_scores2 = cross_val_score(estimator=lreg, X=train[['director2']], y=y, scoring=mean_squared_log_error, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.973936278303664e+22, 30.57244566730471)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the winner is....target encoding.....even if you remove the very strange fold the values are still higher\n",
    "np.mean(dir_scores1), np.mean(dir_scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5).  Define your new version of `X` to be the 'winning' versions of what we've tested so far, as well as all of the original numeric columns\n",
    "\n",
    " - if they need to be onehot encoded, then do so -- make sure to concatenate training and test if you are doing so\n",
    " - Using the `standardscaler()` module, make sure to `fit` it on the training set and `transform` it on the test set to standardize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:23: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "# import the module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# we never actually saved this before\n",
    "train['vote_count'] = np.log1p(train['vote_count'])\n",
    "test['vote_count'] = np.log1p(test['vote_count'])\n",
    "\n",
    "# and grab all numberic columns -- these are the ones that won in this case\n",
    "num_cols = train.select_dtypes(include=np.number).columns.tolist()\n",
    "# except for this one\n",
    "num_cols.remove('revenue')\n",
    "\n",
    "# define X\n",
    "X = train[num_cols]\n",
    "# reduce the test set down to the same number of columns\n",
    "test = test[num_cols]\n",
    "\n",
    "# call fit and transform on the training set\n",
    "X = sc.fit_transform(X)\n",
    "# and then transform the test set as well\n",
    "test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6).  To get an estimate of your models performance, use 10-fold cross validation on your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.835122180347376"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "scores = cross_val_score(estimator=lreg, X=X, y=y, scoring=mean_squared_log_error, cv=10)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7).  Now, before making your final predictions for your test sit, fit the model on all of your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "lreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8).  Make a prediction on your test set, and save the results as a dataframe, using two columns:\n",
    "\n",
    " - **id**:  the id of your test set rows, numbering from 1 - 2000\n",
    " - **predictions**: your corresponding predictions\n",
    " \n",
    "Submit this to a csv file, using the option `index=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "preds = lreg.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and put it into a dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': np.arange(1, 2001),\n",
    "    'prediction': np.expm1(preds)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and if we wanted, we could submit this to a csv file in the following way\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
